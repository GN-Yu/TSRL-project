@misc{lu2023temporal,
      title={Temporal Difference Learning for High-Dimensional PIDEs with Jumps}, 
      author={Liwei Lu and Hailong Guo and Xu Yang and Yi Zhu},
      year={2023},
      eprint={2307.02766},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@article{Zeng2022,
	title = {Deep neural networks based temporal-difference methods for high-dimensional parabolic partial differential equations},
	journal = {Journal of Computational Physics},
	volume = {468},
	pages = {111503},
	year = {2022},
	issn = {0021-9991},
	doi = {https://doi.org/10.1016/j.jcp.2022.111503},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999122005654},
	author = {Shaojie Zeng and Yihua Cai and Qingsong Zou},
	keywords = {High-dimensional PDEs, Forward-backward stochastic differential equations, Neural network, Temporal-difference learning},
	abstract = {Solving high-dimensional partial differential equations (PDEs) is a long-standing challenge, for which classical numerical methods suffer from the well-known curse of dimensionality. In this paper, we propose deep neural networks (NN) based temporal-difference (TD) learning methods for numerically solving high-dimensional parabolic PDEs. To this end, we approximate the solution of the original PDE with an NN function. To calculate this neural network function, we first transform the deterministic parabolic PDE to a forward-backward stochastic differential equations (FBSDE) system with the nonlinear Feynman-Kac formula, and then transform the forward updating process of FBSDE as a Markov reward process (MRP). On this basis, we approximate the solution of the PDE by training an NN function with a reinforcement learning technique described as below: we first discretize the temporal interval to a finite number of time steps and then at each time step, we generate many trajectories and design the loss function as the mean square of temporal difference error on all trajectories. With this loss function, we update the parameters of the NN function with the stochastic gradient descent (SGD) method. Numerical experiments show that comparing to some other existed deep learning methods, our method not only accelerates the computational speed but also improves the computational accuracy. In particular, the relative errors of our algorithm achieve the order of O(10âˆ’4) even the dimension of the problem is as high as 100.}
}