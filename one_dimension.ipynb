{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnfEjmgN+iM1h43VDR+OX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GN-Yu/TSRL-project/blob/main/one_dimension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "import subprocess\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to execute shell command and return output\n",
        "def run_shell_command(cmd):\n",
        "  return subprocess.run(cmd, stdout=subprocess.PIPE, shell=True).stdout.decode('utf-8').strip()\n",
        "\n",
        "# Check CPU\n",
        "cpu_info = run_shell_command(\"cat /proc/cpuinfo | grep 'model name' | uniq\")\n",
        "print(\"CPU:\", cpu_info.split(\":\")[1].strip() if cpu_info else \"Not available\")\n",
        "\n",
        "# Number of CPU Cores\n",
        "num_cores = os.cpu_count()\n",
        "print(\"Number of CPU cores:\", num_cores)\n",
        "\n",
        "# Total RAM\n",
        "ram_info = psutil.virtual_memory()\n",
        "total_ram = ram_info.total / (1024 ** 3)  # Convert bytes to GB\n",
        "print(f\"Total RAM : {total_ram} GB\")\n",
        "\n",
        "# Check GPU\n",
        "gpu_info = !nvidia-smi --query-gpu=gpu_name,memory.total --format=csv\n",
        "if gpu_info and len(gpu_info) > 1:\n",
        "  gpu_name = gpu_info[1].split(',')[0]\n",
        "  gpu_memory_mib = float(gpu_info[1].split()[2])  # Extract the memory in MiB\n",
        "  gpu_memory_gb = gpu_memory_mib / 1024\n",
        "  print(f\"GPU: {gpu_name}, {gpu_memory_gb} GB\")\n",
        "else:\n",
        "  print(\"GPU: Not available\")\n",
        "\n",
        "# TensorFlow info\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"TensorFlow GPU Availability:\", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgd5Aprvn-O3",
        "outputId": "097a9afb-d58e-4cec-d939-c87ff9a00a36"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Number of CPU cores: 2\n",
            "Total RAM : 12.674789428710938 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-105e58084095>:37: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4, 15.0 GB\n",
            "TensorFlow version: 2.15.0\n",
            "TensorFlow GPU Availability: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xVz5AilONonQ"
      },
      "outputs": [],
      "source": [
        "# organize files\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/TSRL_project'):\n",
        "  for filename in filenames:\n",
        "    print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_brownian_motion(num_trajectories, time_steps):\n",
        "  \"\"\"\n",
        "  Function to sample Brownian motion for the specified number of trajectories and time steps.\n",
        "\n",
        "  :param num_trajectories: The number of trajectories to simulate.\n",
        "  :param time_steps: A numpy array of time steps.\n",
        "  :return: A numpy array of shape (num_trajectories, len(time_steps)-1) representing the Brownian increments.\n",
        "  \"\"\"\n",
        "  # Calculate time intervals (delta_t) as differences between consecutive time steps\n",
        "  delta_t = np.diff(time_steps)\n",
        "\n",
        "  # Sample Brownian increments ∆W for each time interval and each trajectory\n",
        "  brownian_increments = np.random.normal(0, np.sqrt(delta_t), (num_trajectories, len(delta_t)))\n",
        "\n",
        "  return brownian_increments\n",
        "\n",
        "def sample_initial_states(num_samples):\n",
        "  # Implement initial state sampling here\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "hVDp3bo8QXp9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Levy jump func in tuple format\n",
        "\n",
        "# def sample_levy_jumps(num_trajectories, lambda_param, mu, sigma, time_horizon):\n",
        "#     \"\"\"\n",
        "#     Function to sample jumps from a Lévy process for the specified number of trajectories.\n",
        "\n",
        "#     :param num_trajectories: The number of trajectories to simulate.\n",
        "#     :param lambda_param: The intensity parameter λ of the Poisson process.\n",
        "#     :param phi_function: The density function ϕ(z) of the jump sizes.\n",
        "#     :param time_horizon: The total time horizon T.\n",
        "#     :return: A list containing tuples (jump_times, jump_sizes) for each trajectory.\n",
        "#     \"\"\"\n",
        "#     jumps = []\n",
        "\n",
        "#     for _ in range(num_trajectories):\n",
        "#         # Step (a): Generate a sequence of exponential distributions with parameter λ\n",
        "#         exponential_samples = np.random.exponential(1/lambda_param, 1000)  # Generate a large number of samples\n",
        "\n",
        "#         # Step (b): Compute the cumulative sum to get the arrival times of the Poisson process\n",
        "#         arrival_times = np.cumsum(exponential_samples)\n",
        "#         arrival_times = arrival_times[arrival_times <= time_horizon]  # Filter times beyond the time horizon\n",
        "\n",
        "#         # Step (c): Sample from ϕ(z) for each arrival time to get the jump sizes\n",
        "#         jump_sizes = np.array([np.random.normal(mu, sigma) for _ in arrival_times])\n",
        "\n",
        "#         jumps.append((arrival_times, jump_sizes))\n",
        "\n",
        "#     return jumps\n",
        "\n",
        "# # Example usage\n",
        "# lambda_param = 0.3  # Intensity parameter λ of the Poisson process\n",
        "# time_horizon = 1    # Total time horizon T\n",
        "# M = 1000  # Number of samples/trajectories\n",
        "# mu = 0.4\n",
        "# sigma = 0.25\n",
        "\n",
        "# np.random.seed(SEED)\n",
        "\n",
        "# # Simulate jumps for a specific number of trajectories\n",
        "# levy_jumps = sample_levy_jumps(M, lambda_param, mu, sigma, time_horizon)\n",
        "\n",
        "# # Example output of the first trajectory's jumps\n",
        "# for i in range(10):\n",
        "#   print(levy_jumps[i])  # (array of arrival times, array of jump sizes)\n",
        "\n",
        "# print(len(levy_jumps))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aXeV-LsjBmaI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_levy_jumps(num_trajectories, lambda_param, mu, sigma, time_horizon, max_jumps=1000):\n",
        "  \"\"\"\n",
        "  Function to sample jumps from a Lévy process for the specified number of trajectories.\n",
        "  Returns a 3D NumPy array with shape (num_trajectories, max_jumps, 2), where the last dimension\n",
        "  contains arrival times and jump sizes respectively.\n",
        "\n",
        "  :param num_trajectories: The number of trajectories to simulate.\n",
        "  :param lambda_param: The intensity parameter λ of the Poisson process.\n",
        "  :param mu: Mean of the normal distribution for jump sizes.\n",
        "  :param sigma: Standard deviation of the normal distribution for jump sizes.\n",
        "  :param time_horizon: The total time horizon T.\n",
        "  :param max_jumps: Maximum number of jumps to consider for each trajectory.\n",
        "  :return: A 3D NumPy array containing (arrival_times, jump_sizes) for each trajectory.\n",
        "  \"\"\"\n",
        "  all_jumps = np.zeros((num_trajectories, max_jumps, 2))\n",
        "\n",
        "  for i in range(num_trajectories):\n",
        "    # Generate a sequence of exponential distributions with parameter λ\n",
        "    exponential_samples = np.random.exponential(1/lambda_param, max_jumps)\n",
        "\n",
        "    # Compute the cumulative sum to get the arrival times of the Poisson process\n",
        "    arrival_times = np.cumsum(exponential_samples)\n",
        "    valid_indices = arrival_times <= time_horizon\n",
        "    arrival_times = arrival_times[valid_indices]\n",
        "\n",
        "    # Sample from the normal distribution for each arrival time to get the jump sizes\n",
        "    jump_sizes = np.random.normal(mu, sigma, arrival_times.size)\n",
        "\n",
        "    all_jumps[i, :arrival_times.size, 0] = arrival_times\n",
        "    all_jumps[i, :arrival_times.size, 1] = jump_sizes\n",
        "\n",
        "  return all_jumps\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# lambda_param = 0.3  # Intensity parameter λ of the Poisson process\n",
        "# time_horizon = 1    # Total time horizon T\n",
        "# M = 1000  # Number of samples/trajectories\n",
        "# mu = 0.4\n",
        "# sigma = 0.25\n",
        "\n",
        "# SEED = 2023\n",
        "# np.random.seed(SEED)\n",
        "\n",
        "# # Simulate jumps for a specific number of trajectories\n",
        "# levy_jumps = sample_levy_jumps(M, lambda_param, mu, sigma, time_horizon)\n",
        "\n",
        "# # Example output of the first trajectory's jumps\n",
        "# for i in range(10):\n",
        "#   print(levy_jumps[i])  # (array of arrival times, array of jump sizes)\n",
        "\n",
        "# print(len(levy_jumps))"
      ],
      "metadata": {
        "id": "8gzLZHcv187u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.stats import norm\n",
        "\n",
        "# z = 0.5 # location to evaluate\n",
        "\n",
        "# # Using scipy.stats\n",
        "# phi = norm.pdf(z, loc=mu, scale=sigma)\n",
        "\n",
        "# print(phi)"
      ],
      "metadata": {
        "id": "o3yAc_n2sbky"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neural_network(input_dim, layer_width=25, output_dim=2):\n",
        "  # Define the input layer\n",
        "  inputs = tf.keras.Input(shape=(input_dim,))\n",
        "\n",
        "  # First linear layer\n",
        "  x = tf.keras.layers.Dense(layer_width, activation='tanh')(inputs)\n",
        "\n",
        "  # Add five residual blocks\n",
        "  for _ in range(5):\n",
        "      y = tf.keras.layers.Dense(layer_width, activation='tanh')(x)\n",
        "      y = tf.keras.layers.Dense(layer_width, activation='tanh')(y)\n",
        "      x = tf.keras.layers.Add()([x, y])\n",
        "\n",
        "  # Final linear layer\n",
        "  outputs = tf.keras.layers.Dense(output_dim)(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "def create_simplified_model(input_dim, output_dim=2):\n",
        "    # Define the input layer\n",
        "    inputs = tf.keras.Input(shape=(input_dim,))\n",
        "\n",
        "    # A single dense layer\n",
        "    outputs = tf.keras.layers.Dense(output_dim)(inputs)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# # Use GPU for model creation\n",
        "# with tf.device('/GPU:0'):\n",
        "#     model = create_neural_network(input_dim=2)\n",
        "\n",
        "# # Prepare input data (example)\n",
        "# input_data = np.array([[0.5, -0.5],\n",
        "#                        [1.0, 0.0],\n",
        "#                        [-0.5, 0.5]])\n",
        "\n",
        "# # Make predictions\n",
        "# output_data = model.predict(input_data)\n",
        "\n",
        "# print(\"Output Data:\")\n",
        "# print(output_data)"
      ],
      "metadata": {
        "id": "dBKB-G3thPfZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.integrate as integrate\n",
        "from scipy.stats import norm\n",
        "\n",
        "def G_function(x, z):\n",
        "  \"\"\"Define the function G(x, z).\"\"\"\n",
        "  return x * np.exp(z)\n",
        "\n",
        "def normal_pdf(z, mu, sigma):\n",
        "  \"\"\"Normal probability density function.\"\"\"\n",
        "  return (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-((z - mu) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "def integral_G_phi(x, mu, sigma, G_func, phi_func):\n",
        "  \"\"\"Calculate the integral of G_func(x, z) * phi_func(z) over R.\"\"\"\n",
        "  def integrand(z):\n",
        "      return G_func(x, z) * phi_func(z, mu, sigma)\n",
        "\n",
        "  # Perform the integration over the entire real line\n",
        "  result, _ = integrate.quad(integrand, -500, 500)\n",
        "  return result\n",
        "\n",
        "def g_function(x):\n",
        "  return x\n",
        "\n",
        "# # Example usage:\n",
        "# mu = 0.4\n",
        "# sigma = 0.25\n",
        "# x = 1  # Example value for x\n",
        "# integral = integral_G_phi(x, mu, sigma, G_function, normal_pdf)\n",
        "# print(integral)\n"
      ],
      "metadata": {
        "id": "yMBpKrUNrvGa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title non-vectorized calcs\n",
        "\n",
        "# np.random.seed(SEED)\n",
        "\n",
        "# # Define Constants\n",
        "\n",
        "# lambda_poisson = 0.3\n",
        "# mu = 0.4\n",
        "# sigma = 0.25\n",
        "\n",
        "# T = 1.0  # Total time\n",
        "# N = 50  # Number of time intervals\n",
        "# # M = 1000  # Number of samples/trajectories\n",
        "# # Iterations = 400  # Number of iterations\n",
        "\n",
        "# # unit tests\n",
        "# M = 5  # Number of samples/trajectories\n",
        "# Iterations = 1  # Number of iterations\n",
        "\n",
        "# initial_learning_rate = 5e-5\n",
        "\n",
        "# # Generate equally spaced time snapshots from 0 to T\n",
        "# time_steps = np.linspace(0, T, N+1)\n",
        "\n",
        "# # Initialize NN model\n",
        "# d = 1 # problem's dimensionality\n",
        "# input_dim = d + 1\n",
        "# with tf.device('/GPU:0'):\n",
        "#   model = create_neural_network(input_dim)\n",
        "\n",
        "\n",
        "# for iteration in range(Iterations):\n",
        "#   brownian_motions = sample_brownian_motion(M, time_steps)\n",
        "#   levy_jumps = sample_levy_jumps(M, lambda_poisson, mu, sigma, T)\n",
        "\n",
        "#   X = [[0 for _ in range(M)] for _ in range(N+1)]\n",
        "#   Y = [[0 for _ in range(M)] for _ in range(N+1)]\n",
        "\n",
        "#   for timestep, time in enumerate(time_steps):\n",
        "#     if timestep == 0:\n",
        "#       X[timestep] = [1 for _ in range(M)]  # Initial value for PIDE\n",
        "#     else:\n",
        "#       timestep_pre = timestep - 1\n",
        "#       time_pre = time_steps[timestep_pre]\n",
        "#       time_diff = time - time_pre\n",
        "\n",
        "#       for trajectory in range(M):\n",
        "#         arrival_times, jump_sizes = levy_jumps[trajectory]\n",
        "#         mask = (arrival_times > time_pre) & (arrival_times <= time)\n",
        "\n",
        "#         X[timestep][trajectory] = X[timestep_pre][trajectory] + G_function(X[timestep_pre][trajectory], jump_sizes[mask]).sum() - time_diff * lambda_poisson * integral_G_phi(X[timestep_pre][trajectory], mu, sigma, G_function, normal_pdf)\n",
        "\n"
      ],
      "metadata": {
        "id": "fU6Q2QUrlRGM",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "SEED = 2023\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Constants\n",
        "lambda_poisson = 0.3\n",
        "mu = 0.4\n",
        "sigma = 0.25\n",
        "T = 1.0  # Total time\n",
        "N = 50   # Number of time intervals\n",
        "# M = 1000  # Number of samples/trajectories\n",
        "# Iterations = 400  # Number of iterations\n",
        "\n",
        "# unit tests\n",
        "M = 5  # Number of samples/trajectories\n",
        "Iterations = 2  # Number of iterations\n",
        "\n",
        "initial_learning_rate = 5e-5\n",
        "\n",
        "\n",
        "# Generate equally spaced time snapshots from 0 to T\n",
        "time = np.linspace(0, T, N+1)\n",
        "\n",
        "# Initialize NN model\n",
        "d = 1  # problem's dimensionality\n",
        "input_dim = d + 1\n",
        "with tf.device('/GPU:0'):\n",
        "  model = create_neural_network(input_dim)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Ensure the model is set to training mode\n",
        "model.trainable = True\n",
        "\n",
        "# Randomly generate terminal state (normal with mean of intial value)\n",
        "X_terminal = np.random.normal(1, 0.1, M)\n",
        "# Convert X_terminal to a TensorFlow Tensor\n",
        "X_terminal_tensor = tf.convert_to_tensor(X_terminal)\n",
        "\n",
        "for iteration in range(Iterations):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    brownian_motions = sample_brownian_motion(M, time)\n",
        "    levy_jumps = sample_levy_jumps(M, lambda_poisson, mu, sigma, T)\n",
        "\n",
        "    # Initialize arrays for X and Y\n",
        "    X = np.zeros((N+1, M))\n",
        "    Y = np.zeros((N+1, M))\n",
        "\n",
        "    # Set initial and terminal value for X\n",
        "    X[0, :] = 1\n",
        "    X[N, :] = X_terminal\n",
        "\n",
        "    # Iterate over time steps\n",
        "    for timestep in range(N):\n",
        "      time_diff = time[timestep + 1] - time[timestep]\n",
        "\n",
        "      # Mask for non-zero jumps that occur in the current time interval\n",
        "      masks = (levy_jumps[:, :, 0] > time[timestep]) & (levy_jumps[:, :, 0] <= time[timestep + 1])\n",
        "\n",
        "      # Vectorized computation of jump contributions for each trajectory\n",
        "      X_current = X[timestep, :][:, np.newaxis]  # Reshape X to for broadcasting\n",
        "      jump_contributions = np.sum(G_function(X_current, levy_jumps[:, :, 1]) * masks, axis=1)\n",
        "\n",
        "      # Apply integral_G_phi element-wise\n",
        "      integral_contributions = np.array([time_diff * lambda_poisson * integral_G_phi(x, mu, sigma, G_function, normal_pdf) for x in X[timestep, :]])\n",
        "\n",
        "      # Calculate X for next time step\n",
        "      X[timestep + 1, :] = X[timestep, :] + jump_contributions - integral_contributions\n",
        "\n",
        "      NN_outputs_current = model(np.stack([np.full(M, time[timestep]), X[timestep, :]], axis=1)) # can be optimized for calc\n",
        "\n",
        "      NN_outputs_next = model(np.stack([np.full(M, time[timestep+1]), X[timestep+1, :]], axis=1)) # can be optimized for calc\n",
        "\n",
        "      TD_error_jump_contribution = np.zeros(M)\n",
        "\n",
        "      for trajectory in range(M):\n",
        "        # Filter levy_jumps for the current trajectory and time interval\n",
        "        relevant_jumps = [jump for jump in levy_jumps[trajectory] if time[timestep] < jump[0] <= time[timestep + 1]]\n",
        "\n",
        "        # If there are no relevant jumps, set TD_error_jump_contribution for this trajectory to 0\n",
        "        if not relevant_jumps:\n",
        "          TD_error_jump_contribution[trajectory] = 0\n",
        "        else:\n",
        "          TD_error_jump_contribution[trajectory] = sum(\n",
        "              model(np.array([[time[timestep], X[timestep, trajectory] + G_function(X[timestep, trajectory], jump[1])]]))[0,0]\n",
        "              - model(np.array([[time[timestep], X[timestep, trajectory]]]))[0,0]\n",
        "              for jump in relevant_jumps\n",
        "          )\n",
        "\n",
        "      # Loss 1: TD error\n",
        "      TD_error = tf.reduce_mean(tf.square(TD_error_jump_contribution - time_diff * NN_outputs_current[:, 1] + NN_outputs_current[:, 0] - NN_outputs_next[:, 0]))\n",
        "\n",
        "      # Loss 4: N1 and N2 constraint error\n",
        "      constraint_error = tf.abs(tf.reduce_mean(TD_error_jump_contribution - time_diff * NN_outputs_current[:, 1]))\n",
        "\n",
        "      if timestep == N-1:\n",
        "        X_terminal_tensor = tf.convert_to_tensor(X[N, :])\n",
        "\n",
        "      terminal_input = tf.convert_to_tensor(np.stack([np.full(M, T), X_terminal_tensor], axis=1))\n",
        "\n",
        "      tape.watch(terminal_input)  # Ensure terminal_input is being watched\n",
        "      tape.watch(X_terminal_tensor)  # Ensure X_terminal_tensor is being watched\n",
        "\n",
        "      NN_outputs_terminal = model(terminal_input)\n",
        "\n",
        "      N1_gradient_terminal = tape.gradient(NN_outputs_terminal[:, 0], terminal_input)[:, 1]\n",
        "\n",
        "      g_X_terminal = tf.cast(g_function(X_terminal_tensor), tf.float32)\n",
        "      g_gradient = tape.gradient(g_X_terminal, X_terminal_tensor)\n",
        "\n",
        "      # Loss 2: Y terminal value error\n",
        "      Y_terminal_error = tf.reduce_mean(tf.square(NN_outputs_terminal[:, 0] - g_X_terminal)) / N\n",
        "\n",
        "      # Loss 3: Y terminal value derivative error\n",
        "      Y_teriminal_deriv_error = tf.cast(tf.reduce_mean(tf.square(N1_gradient_terminal - g_gradient)) / N, tf.float32)\n",
        "\n",
        "      loss = TD_error + Y_terminal_error + Y_teriminal_deriv_error + constraint_error\n",
        "\n",
        "      print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyM_H4IBzjQG",
        "outputId": "566003ef-10e3-4247-ed17-c51e7481d8bd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.00797916, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0076852306, shape=(), dtype=float32)\n",
            "tf.Tensor(0.007381106, shape=(), dtype=float32)\n",
            "tf.Tensor(0.007067344, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0067445654, shape=(), dtype=float32)\n",
            "tf.Tensor(0.006413544, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0060751257, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0057302844, shape=(), dtype=float32)\n",
            "tf.Tensor(0.005623257, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0059728855, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0063265455, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0066829687, shape=(), dtype=float32)\n",
            "tf.Tensor(0.007040894, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0073990664, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0077563003, shape=(), dtype=float32)\n",
            "tf.Tensor(0.008111456, shape=(), dtype=float32)\n",
            "tf.Tensor(0.008463527, shape=(), dtype=float32)\n",
            "tf.Tensor(0.00881158, shape=(), dtype=float32)\n",
            "tf.Tensor(0.009154849, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0094927, shape=(), dtype=float32)\n",
            "tf.Tensor(0.009824661, shape=(), dtype=float32)\n",
            "tf.Tensor(0.01015044, shape=(), dtype=float32)\n",
            "tf.Tensor(0.01046991, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0107831275, shape=(), dtype=float32)\n",
            "tf.Tensor(0.011090374, shape=(), dtype=float32)\n",
            "tf.Tensor(0.011392007, shape=(), dtype=float32)\n",
            "tf.Tensor(0.011688644, shape=(), dtype=float32)\n",
            "tf.Tensor(0.011980997, shape=(), dtype=float32)\n",
            "tf.Tensor(0.012269926, shape=(), dtype=float32)\n",
            "tf.Tensor(0.012556414, shape=(), dtype=float32)\n",
            "tf.Tensor(0.012841493, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0131262615, shape=(), dtype=float32)\n",
            "tf.Tensor(0.013411859, shape=(), dtype=float32)\n",
            "tf.Tensor(0.10086833, shape=(), dtype=float32)\n",
            "tf.Tensor(0.013876582, shape=(), dtype=float32)\n",
            "tf.Tensor(0.014196127, shape=(), dtype=float32)\n",
            "tf.Tensor(0.014519934, shape=(), dtype=float32)\n",
            "tf.Tensor(0.014848363, shape=(), dtype=float32)\n",
            "tf.Tensor(0.015181683, shape=(), dtype=float32)\n",
            "tf.Tensor(0.01552007, shape=(), dtype=float32)\n",
            "tf.Tensor(0.015863575, shape=(), dtype=float32)\n",
            "tf.Tensor(0.01621215, shape=(), dtype=float32)\n",
            "tf.Tensor(0.016565643, shape=(), dtype=float32)\n",
            "tf.Tensor(0.016923804, shape=(), dtype=float32)\n",
            "tf.Tensor(0.017286327, shape=(), dtype=float32)\n",
            "tf.Tensor(0.017652828, shape=(), dtype=float32)\n",
            "tf.Tensor(0.018022848, shape=(), dtype=float32)\n",
            "tf.Tensor(0.018395955, shape=(), dtype=float32)\n",
            "tf.Tensor(0.018771634, shape=(), dtype=float32)\n",
            "tf.Tensor(0.20415913, shape=(), dtype=float32)\n",
            "tf.Tensor(0.033752836, shape=(), dtype=float32)\n",
            "tf.Tensor(0.033458903, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03315478, shape=(), dtype=float32)\n",
            "tf.Tensor(0.032841016, shape=(), dtype=float32)\n",
            "tf.Tensor(0.032518238, shape=(), dtype=float32)\n",
            "tf.Tensor(0.032187216, shape=(), dtype=float32)\n",
            "tf.Tensor(0.0318488, shape=(), dtype=float32)\n",
            "tf.Tensor(0.031503957, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03139693, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03174656, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03210022, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03245664, shape=(), dtype=float32)\n",
            "tf.Tensor(0.032814566, shape=(), dtype=float32)\n",
            "tf.Tensor(0.033172738, shape=(), dtype=float32)\n",
            "tf.Tensor(0.14568007, shape=(), dtype=float32)\n",
            "tf.Tensor(0.033662595, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03398946, shape=(), dtype=float32)\n",
            "tf.Tensor(0.034315318, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03463962, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03496191, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03528185, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03559924, shape=(), dtype=float32)\n",
            "tf.Tensor(0.035913996, shape=(), dtype=float32)\n",
            "tf.Tensor(0.036226172, shape=(), dtype=float32)\n",
            "tf.Tensor(0.036535975, shape=(), dtype=float32)\n",
            "tf.Tensor(0.036843672, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03714971, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03745458, shape=(), dtype=float32)\n",
            "tf.Tensor(0.037758894, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03806333, shape=(), dtype=float32)\n",
            "tf.Tensor(0.038368557, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03867532, shape=(), dtype=float32)\n",
            "tf.Tensor(0.03898434, shape=(), dtype=float32)\n",
            "tf.Tensor(0.039296314, shape=(), dtype=float32)\n",
            "tf.Tensor(0.039611883, shape=(), dtype=float32)\n",
            "tf.Tensor(0.039931625, shape=(), dtype=float32)\n",
            "tf.Tensor(0.04025603, shape=(), dtype=float32)\n",
            "tf.Tensor(0.04058548, shape=(), dtype=float32)\n",
            "tf.Tensor(0.040920246, shape=(), dtype=float32)\n",
            "tf.Tensor(0.041260496, shape=(), dtype=float32)\n",
            "tf.Tensor(0.041606266, shape=(), dtype=float32)\n",
            "tf.Tensor(0.041957475, shape=(), dtype=float32)\n",
            "tf.Tensor(0.042313963, shape=(), dtype=float32)\n",
            "tf.Tensor(0.042675428, shape=(), dtype=float32)\n",
            "tf.Tensor(0.043041512, shape=(), dtype=float32)\n",
            "tf.Tensor(0.043411795, shape=(), dtype=float32)\n",
            "tf.Tensor(0.043785743, shape=(), dtype=float32)\n",
            "tf.Tensor(0.044162866, shape=(), dtype=float32)\n",
            "tf.Tensor(0.044542585, shape=(), dtype=float32)\n",
            "tf.Tensor(0.019066144, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your multi-dimensional function\n",
        "def my_custom_function(x, t):\n",
        "    # Example computation; replace this with your actual function\n",
        "    return t * x\n",
        "\n",
        "# Example multi-dimensional input\n",
        "# Let's assume the input has shape [batch_size, input_dim]\n",
        "# For example, batch_size = 3, input_dim = 2\n",
        "x = tf.Variable([[1.0, 2.0], [1.0, 2.0], [1.0, 2.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(x)  # Ensure x is being watched for gradient computation\n",
        "    output = my_custom_function(x, 2)\n",
        "\n",
        "# Compute the gradient of the output with respect to the input\n",
        "grads = tape.gradient(output, x)\n",
        "\n",
        "print(grads)  # This will print the gradient tensor\n"
      ],
      "metadata": {
        "id": "88AbnDQmKrYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplified_input = tf.constant([[T, 1.0]], dtype=tf.float32)\n",
        "with tf.GradientTape() as tape:\n",
        "    tape.watch(simplified_input)\n",
        "    simplified_output = model(simplified_input)\n",
        "    simplified_gradient = tape.gradient(simplified_output, simplified_input)\n",
        "print(\"Simplified Gradient:\", simplified_gradient)\n"
      ],
      "metadata": {
        "id": "7zI5lBaZFSYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example neural network with multiple outputs\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(2)  # Let's say we have 2 outputs\n",
        "])\n",
        "\n",
        "# Example input\n",
        "x = tf.Variable([[0.1], [0.2], [0.3]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    tape.watch(x)\n",
        "    outputs = model(x)\n",
        "\n",
        "# Assuming outputs is a tensor of shape [batch_size, num_outputs]\n",
        "gradients = []\n",
        "for i in range(outputs.shape[1]):  # Loop over each output\n",
        "    # Compute the gradient of the i-th output with respect to x\n",
        "    grad = tape.gradient(outputs[:, i], x)\n",
        "    gradients.append(grad)\n",
        "\n",
        "# gradients now contains the gradients of each output with respect to x\n"
      ],
      "metadata": {
        "id": "tPScyrQph9kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gl9c_YWllBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}