\documentclass[12pt,letterpaper,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{breqn}
\usepackage{extarrows}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{bbm}

\usepackage{geometry}
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\fancyhf{}
%\renewcommand\headrulewidth{0.7pt} %initially 0.4pt
%\cfoot{\thepage}
%\fancyhead[C]{TSRL Interview}
%\fancyhead[L]{Question 3 Solutions}
%\fancyhead[R]{Guoning Yu}

%\usepackage{mathtools}
%\usepackage{ntheorem}
\usepackage{enumerate}
%%\usepackage[ruled]{algorithm2e}
%
%\usepackage{booktabs}

\title{Project Proposal for TSRL Interview Question 3}
\author{Guoning Yu}
\date{Winter 2023}

%\bibliographystyle{plain}

\begin{document}
	
\baselineskip 0.6cm
\maketitle

\section{Introduction}
	This question is about reading the paper \textit{Temporal Difference Learning for High-Dimensional PIDEs with Jumps, Liwei Lu, Hailong Guo, Xu Yang, Yi Zhu, 2023}~\cite{lu2023temporal} and replicate the reinforcement learning algorithm in it. 
	
	A \textit{Partial Integro-Differential Equation (PIDE)} is a type of equation that combines features of both partial differential equations (PDEs) and integral equations. It has wide-spread applications in various scenarios, and particularly in finance. In a PIDE, the unknown function is defined over a domain and the equation involves both partial derivatives and integral operators acting on this function. More specifically, a PIDE typically takes the  general form
	\begin{equation}
		\frac{\partial u}{\partial t}+\mathcal{L} u+\mathcal{I} u=f,
	\end{equation}
	in which $u = u(t, \textbf{x})$ is the unknown function with a time variable $ t \in [0, T] $ and a spatial variable $ \textbf{x}\in \mathbb{R}^d $, $\mathcal{L}$ denotes a differential operator with respect to the spatial variable $ \textbf{x}$, and $\mathcal{I}$ represents an integral operator of $u$ with respect to $ \textbf{x} $.
	
	\textit{Jumps} refer to sudden, significant changes in the price of an asset. These can be caused by various events such as economic announcements, geopolitical events, or sudden market movements. Unlike the normal price fluctuations assumed in models like Black-Scholes, jumps represent discontinuities, where the asset price changes significantly in a very short period, often unpredictably. 
		
	PIDEs are particularly common in financial mathematics for modeling options pricing in markets with jumps or other non-continuous features, as they can capture both the local changes (via the differential part) and the global or aggregate effects (via the integral part), which addresses the limitations of models like Black-Scholes in markets with jumps.
	
	In the context of PIDEs, the integral part, namely, the ``non-local'' term typically has the general form
	$$
	\mathcal{I}(u)(t, x)=\int_{\Omega} K(x, y, t) u(y, t) d y,
	$$
	where $\Omega$ is the domain of integration, and $K(x, y, t)$ is a kernel function that specifies how values of $u$ at different points $y$ contribute to the integral at the point $x$. The kernel often encapsulates the nature of the non-local interactions.
	
	However, solving PIDEs is extremely hard in high-dimensional cases given a large number of underlying assets. There are previous work using deep neural networks (NN) to solve PDEs, while the results on solving PIDEs with NN method are not as extensive.
	
	The paper is about solving PIDEs numerically. A group of L\'evy-type forward-backward stochastic processes is introduced based on the target PIDE. It gives an algorithm that ... They used a method of Temporal Difference Learning under the Reinforcement Learning framework.  The result presented in the paper is ... The significance of their algorithm is ...

	
\section{Typos and Errors in Paper}
	The typos and errors in the paper are listed as follows.
	\begin{enumerate}
		\item In Equation (2.9), on the left side of the equation, the subscription is wrong: $ X_{t_{n}+1} $ should be $ X_{t_{n+1}} $.
		\item In Fig. 1, the output $ \mathcal{N}_2 $ should be the integral of $ \cdots\nu(dz) $, not $ \cdots dz $
		\item In conclusion section: "the future work" should be "future work".
	\end{enumerate}


\section{Unclear Parts}
	The parts that I had hard time understanding are:
	
	\begin{enumerate}
		\item 
	\end{enumerate}

\section{Replication Results}
	I used TensorFlow to replicate ... The project implementation is in the \href{https://github.com/GN-Yu/TSRL-project}{GitHub Repo}.
	
	In my replication, I used the following techniques.
	
	The results of my replication are ... 
	
	Comparing with the paper's original results, ....
	
	
	
\section{Notes for Important Concepts}

\subsection{Temporal Difference Learning}
	Temporal Difference (TD) Learning is a Reinforcement Learning (RL) method with form of bootstrapping. Bootstrapping is a resampling method used in model selection, assessment and estimation of statistical properties. To do bootstrapping, we draw a sample from your dataset with replacement repeatedly until we create a new sample of the same size (sometimes larger or smaller) as the original dataset. TD Learning is a method for learning the value function, which is a prediction of future rewards, based on the idea of updating estimates based partly on other learned estimates. This is where the bootstrapping aspect comes in.
	
	The core of TD learning is the \textit{TD error}, a difference between estimated values at successive time steps. In its simplest form TD(0), or Temporal Difference Learning with a look-ahead of zero, the TD error for state-value estimation is given by:
	\begin{equation}\label{eq:TD_error}
		\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t),
	\end{equation}
	where \( R_{t+1} \) is the reward received at time \( t+1 \), \( \gamma \) is the discount factor, \( V(S_t) \) is the current estimate of the state value, and \( V(S_{t+1}) \) is the estimate of the next state's value.
	
	In TD learning, the value function for a state is updated incrementally after each step. The update is done in the direction of the TD error. For example, the value function update rule in TD(0) is:
	\begin{equation}
		V(S_t) \leftarrow V(S_t) + \alpha \delta_t,
	\end{equation}
	where \( \alpha \) is the learning rate.
	
	TD methods do not require waiting until the end of an episode to perform updates, making them more suitable for continuing environments. They can learn online after every step.
	
\subsection{Combination of Loss Functions}
	The total loss of the model in the paper is combined with four loss functions at each time step.
	
	\textbf{Loss 1: } TD error -- the error from \autoref{eq:TD_error}.
	
	\textbf{Loss 2: } Termination condition error -- the cumulative reward process $ Y_t $ needs to satisfy $ Y_T = g(X_T) $, and so we calculate the error of such termination condition by taking the Mean Square Error (MSE) between $ g(x_T^j) $ and the estimation of $ Y_T $ which is $ \mathcal{N}_1(T,x_T^j) $. Note that since we do not requite the simulation of entire trajectory, the terminal state $ X_T $ is taken from the previous iteration in the calculation. 
	
	\textbf{Loss 3: } Termination condition gradients error -- the gradient of $ Y_T $ and $ g(X_T) $ should also be equal, therefore, the MSE of the gradients of the two estimations in Loss 2 is taken as another loss.
	
	\textbf{Loss 4: } Neural network approximation error -- 
	
	
\subsection{L\'evy-type Forward-backward Stochastic Processes}
	L\'evy process is a stochastic process with independent, stationary increments. More specifically, it is a stochastic process $X=\left\{X(t): t \geq 0\right\}$ that satisfies the following properties:
	\begin{enumerate}
		\item \textbf{Starts at Zero: }$X(0)=0$ almost surely;
		\item \textbf{Independent increments: }For any $0 \leq t_1<t_2<\cdots<t_n<\infty, X(t_2)-X(t_1), X(t_3)-X(t_2), \ldots, X(t_n)-X(t_{n-1})$ are mutually independent;
		\item \textbf{Stationary increments: }For any $s<t, X(t)-X(s)$ is equal in distribution to $X(t-s)$;
		\item \textbf{Continuous in probability: }For any $\varepsilon>0$ and $t \geq 0$, $\lim \limits_{h \rightarrow 0} P\left(\left|X(t+h)-X(t)\right|>\varepsilon\right)=0$.
	\end{enumerate}
	 The most well-known L\'evy processes are the Brownian motion (Wiener process), which is a process without jumps, and the Poisson process, which can include jumps. A L\'evy process $X=\left\{X(t): t \geq 0\right\}$ defined as above is a Brownian motion if for any $t\ge 0$,
	 $$ X(t) \sim \text{N}(\mu, \sigma^2), $$
	 where $ \mu = 0 $ and $ \sigma = t $.
	 A L\'evy process $X=\left\{X(t): t \geq 0\right\}$ defined as above is a Poisson process if for any $t\ge 0$,
	 $$ X(t) \sim \text{Pois}(\Lambda), $$ 
	 where $ \Lambda = \lambda(t-s) $, and $ \lambda $ is called rate or intensity.
	 
	 The It\^o's formula is ...
	 
	 By applying the L\'evy type stochastic integrals and the It\^o's formula to equation (2.6), we can derive equation (2.8). (needs to check)  The system of (2.6) and (2.8) are referred to as the \textit{forward-backward stochastic differential equation (FBSDE) system}. Equation (2.6) is called the ``forward'' equation since it describes the evolution of the state variable \( X_t \) in a forward manner in time, starting from an initial condition at \( t = 0 \) and moving towards \( t = T \). The dynamics of \( X_t \) are influenced by a deterministic term (\( b(X_t) \)), stochastic term (Brownian motion \( dW_t \)), and a jump term involving the LÃ©vy process \( \int_{\mathbb{R}^d} G(X_t, z) \tilde{N}(dt, dz) \). Equation (2.8) describes the dynamics of the processes \( Y_t \), \( Z_t \), and \( U_t \), which are connected to the value function \( u(t, X_t) \) and its derivatives. Unlike the forward equation, it is called a ``backward'' equation as it starts with a terminal condition at time \( T \) (i.e., \( Y_T = g(X_T) \)) and then moves backward in time. This backward structure is intrinsic to many problems in stochastic control and finance, where the final payoff or condition is known, and the goal is to determine the optimal strategy or value function backward in time.
	 
	 The processes of $ (X_t, Y_t, Z_t, U_t) $ satisfying equation (2.8) provides a foundation for assessing the accuracy of the approximate solution $ u(t,x) $ to the PIDE (2.4), which is the key idea of the whole paper. The discrete version stated by equation (2.9) and (2.10) can be applied to simulate for numerical approximation.
	 
\section{Further Questions}
	There is an important reference~\cite{Zeng2022} that also uses such approach to solve high-dimensional parabolic PDEs.


\bibliographystyle{siam}
\bibliography{ref.bib}
\end{document}